{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [notebook](https://crosscompute.com/n/Sfknz2iPxwDdsLTcJvmWQeYETwYkhwIb) was written by Aida Shoydokova in preparation for a workshop on [Computational Approaches to Fight Human Trafficking](https://www.meetup.com/spatiotemporal-analysis-for-community-health-and-safety/events/244179401).\n",
    "\n",
    "\n",
    "# **Information extraction**\n",
    "Build an information extraction system and populate a well-organized database \n",
    "\n",
    "\n",
    "### Extract below information from unstructured data:\n",
    "\n",
    "* **Category and Subcategory of Human Trafficking**:\n",
    "    * Sex Trafficking\n",
    "        * Adult Sex Trafficking\n",
    "        * Child Sex Trafficking\n",
    "    * Labor\n",
    "        * Bonded Labor or Debt Bondage\n",
    "        * Domestic Servitude\n",
    "        * Forced Child Labor\n",
    "        * Unlawful Recruitment and Use of Child Soldiers\n",
    "    * Organ Removal\n",
    "    * Not Human Trafficking Article\n",
    "    * Something else\n",
    "* **Date**\n",
    "    * Publication Date \n",
    "    * Conviction Date\n",
    "    * Incident Start Date\n",
    "    * Incident End Date \n",
    "* **Geo-Political Location**    \n",
    "    * Country where a trafficker was operating\n",
    "    * Country of origin of victim\n",
    "    * Country of origin of trafficker\n",
    "    * State/Province where a trafficker was operating\n",
    "    * State/Province of origin of victim\n",
    "    * State/Province of trafficker\n",
    "    * City where a trafficker was operating\n",
    "    * City of origin of victim\n",
    "    * City of origin of trafficker\n",
    "* **\"ID Information\"** - information that might help to dedupe incidents\n",
    "    * Trafficker name\n",
    "    * Victim Name\n",
    "* **Demographic Information** \n",
    "    * Victim race\n",
    "    * Trafficker race\n",
    "    * Ethnicity of trafficker\n",
    "    * Ethnicity of victim\n",
    "    * Victim Age\n",
    "    * Trafficker Age\n",
    "    * Victim Gender\n",
    "    * Trafficker Gender\n",
    "    * Victim's Level of education\n",
    "    * Trafficker's Level of education\n",
    "    * Occupation of trafficker\n",
    "    * Prior occupation of victim\n",
    "    * Post occupation of victim\n",
    "    * Victim's Income level\n",
    "    * Trafficker's Income level\n",
    "    * Victim's Marital status\n",
    "    * Trafficker's Martial status\n",
    "    * Religion of victim\n",
    "    * Religion of trafficker\n",
    "* **Length of Human Trafficking**\n",
    "    * How long was a victim harbored?\n",
    "    * How long did a trafficker operate?\n",
    "* **How was a victim recruited?** \n",
    "    * threat\n",
    "    * coercion\n",
    "    * abduction\n",
    "    * fraud/deceit/deception\n",
    "    * abuse of power\n",
    "    * something else\n",
    "* **How was a victim transported/transferred?**\n",
    "* **How did a victim escape?**\n",
    "* **Is it a repeat victim?**\n",
    "* **Is it a repeat trafficker?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOJ Press Releases\n",
    "Build a system that extracts information from DOJ Press Releases\n",
    "\n",
    "### Loading all needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import operator\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# enable IPython to display matplotlib graphs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from the database to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from os.path import basename, join\n",
    "from pandas import read_csv\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "url = (\n",
    "    'https://www.dropbox.com/s/74mgua40dhg6acq/'\n",
    "    'human-trafficking-usa-doj-20171111-1730.csv-sample-100.xz?dl=1')\n",
    "pr = read_csv(url, compression='xz')\n",
    "len(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\n",
    "    'https://www.dropbox.com/s/zr1tem2w4w1ocjz/'\n",
    "    'human-trafficking-usa-doj-20171111-1730.csv.xz?dl=1')\n",
    "pr = read_csv(url, compression='xz')\n",
    "len(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=50\n",
    "print(pr.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime.datetime.fromtimestamp(pr.published_time.min()).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(x):\n",
    "    return datetime.datetime.fromtimestamp(x).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime.datetime.fromtimestamp(1347517370)\n",
    "import datetime\n",
    "\n",
    "print(\n",
    "    '1. The earliest date:', get_date(pr.published_time.min()), \n",
    "    '; Last pulled date:', get_date(pr.published_time.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(i):\n",
    "    try:\n",
    "        if np.isnan(i):\n",
    "            return 0\n",
    "    except TypeError:\n",
    "        return len(i.split(';'))\n",
    "    \n",
    "pr['topic_names'].map(lambda x: f(x))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(i):\n",
    "    try:\n",
    "        if np.isnan(i):\n",
    "            return None\n",
    "    except TypeError:\n",
    "        return len(i.split(';'))\n",
    "    \n",
    "pr['# topics'] = pr['topic_count'] = pr['topic_names'].map(\n",
    "    lambda x: f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr['# topics']=pr['topic'].map(lambda x: int(len(x)) if len(x) else None)\n",
    "\n",
    "print(\n",
    "    '2. Number of records:',len(pr), \n",
    "    '; Percentage of records that have topics:',\n",
    "    \"{0:.0f}%\".format(100*len(pr[~pd.isnull(\n",
    "        pr['topic_count'])])/len(pr)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    '3. Number of records with empty body:',\n",
    "    len(pr[pr['body'].map(\n",
    "        lambda x: True if pd.isnull(x) else False)]),\n",
    "    '; Number of records with empty title:',\n",
    "        len(pr[pr['title'].map(\n",
    "            lambda x: False if len(x) else True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Distribution of Number of Topics per a document')\n",
    "dist_n_topics = pr['# topics'].value_counts(sort=True)\n",
    "print(dist_n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr['topic_names'].dropna()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr['topic_names'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dist_topics = count_by_topic = defaultdict(int)\n",
    "for x in pr['topic_names']:\n",
    "    if pd.isnull(x):\n",
    "        continue\n",
    "    for topic_name in x.split(';'): \n",
    "        count_by_topic[topic_name.strip()] += 1\n",
    "count_by_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print('Topic Distribution')\n",
    "dist_topics = {}\n",
    "for row in pr['topic_names']:\n",
    "     for topic in row:\n",
    "        dist_topics[topic['name']] = dist_topics.get(topic['name'], 0) + 1\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame.from_records(list(dist_topics.items()), columns=['Topic Name', '# Documents']) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "X = np.arange(len(dist_topics))\n",
    "plt.bar(X, dist_topics.values(), align='center', width=0.5)\n",
    "_xticks = [list(dist_topics.keys())[i] if i in [21,37] else '' for i in X ]\n",
    "plt.xticks(X, _xticks)\n",
    "ymax = max(dist_topics.values()) + 1\n",
    "plt.ylim(0, ymax)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of cases that have no published_time\n",
    "len(pr[pr['published_time'] == None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr['published_time'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr['topic_count'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of cases that lack both published_time and topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pr['published_time'].isnull() & pr['topic_count'].isnull()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Empty Topic Distribution by Year')\n",
    "plt.figure(2)\n",
    "pr['Year'] = pr['published_time'].map(\n",
    "    lambda x: None if pd.isnull(x) else get_date(x).year)\n",
    "empty = pr[pd.isnull(pr['# topics'])]['Year'].value_counts()\n",
    "plt.bar(empty.index.values, empty.values)\n",
    "_xticks = ['' if x%2 else int(x) for x in empty.index.values]\n",
    "plt.xticks(empty.index.values, _xticks)\n",
    "ymax = max(empty.values) + 1\n",
    "plt.ylim(0, ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting data by only Human Trafficking related articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr['topic_names'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_trafficking_in_topic(x):\n",
    "    ht = ''\n",
    "    if pd.isnull(x):\n",
    "        return ''        \n",
    "    for e in x.split(';'):\n",
    "        if re.search(r'human\\s+traffic', e ,re.I):\n",
    "            ht += ';' + e\n",
    "    return ht \n",
    "\n",
    "pr['ht_in_topic'] = pr['topic_names'].map(human_trafficking_in_topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we decided to trust to the original labeling \n",
    "# if there is a 'human trafficking' in the body, but the topic does not indicate that\n",
    "# then we believe the press release is not about human trafficking\n",
    "def human_trafficking_in_body_title_empty_topic(x):\n",
    "    ht = ''\n",
    "    topic_names = x['topic_names']\n",
    "    if pd.isnull(topic_names):\n",
    "        return ''        \n",
    "    if not len(topic_names):\n",
    "        if re.search(r'human\\s+traffic',str(x['title']),re.I):\n",
    "            ht += ';' + r'human\\s+traffic'\n",
    "        elif re.search(r'human\\s+traffic',str(x['body']),re.I):\n",
    "            ht += ';' + r'human\\s+traffic'\n",
    "    return ht\n",
    "\n",
    "pr['ht_in_body_title'] = pr.apply(human_trafficking_in_body_title_empty_topic,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new Human Trafficking dataframe\n",
    "pr_ht = pr[(pr['ht_in_topic'] != '') | (pr['ht_in_body_title'] != '')]\n",
    "# statistics\n",
    "print('human trafficking in topic',len(pr[pr['ht_in_topic']!='']))\n",
    "print('human trafficking in body or title with empty topic',len(pr[pr['ht_in_body_title']!='']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the body and title\n",
    "\n",
    "80% of time we spend cleaning the data, remaining 20% we spend complaining that data could've been clean better\n",
    "\n",
    "* Body of press releases have lxml code, let's remove them\n",
    "<img alt=\"DOJ Raw Data\" src=\"resources/DOJ_raw_view.png\" width=\"600px\" />\n",
    "\n",
    "* Articles or press releases usually have a location at the beginning of the article\n",
    "\n",
    "<center>**Press Release Example and Template**</center>\n",
    "<img alt=\"example-press-release\" src=\"resources/example_press_release.png\" width=\"400px\" />\n",
    "<center>**New York Times Article**</center>\n",
    "<img alt=\"NYT\" src=\"resources/NYT.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_location_at_beginning(x):\n",
    "    if re.search(r'^\\s*\\w+\\s*(\\s*\\w+\\s*){0,1}(,\\s*\\w+\\s*[\\.\\s]*){0,1}[\\-\\u2011\\u2012\\u2013\\u2014\\u2015]+',x):\n",
    "        # HYPHEN, NON-BREAKING HYPHEN, FIGURE DASH, EN DASH, EM DASH, HORIZONTAL BAR\n",
    "        return re.sub(r'^\\s*\\w+\\s*(\\s*\\w+\\s*){0,1}(,\\s*\\w+\\s*[\\.\\s]*){0,1}[\\-\\u2011\\u2012\\u2013\\u2014\\u2015]+','',x)\n",
    "    elif re.search(r'^\\s*WASHINGTON\\s*[,\\s]*D\\.{0,1}\\s*C\\.{0,1}\\s*[\\-\\u2011\\u2012\\u2013\\u2014\\u2015]+',x,re.I):\n",
    "        return re.sub(r'^\\s*WASHINGTON\\s*[,\\s]*D\\.{0,1}\\s*C\\.{0,1}\\s*[\\-\\u2011\\u2012\\u2013\\u2014\\u2015]+','',x,re.I)    \n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "# remove lxmls\n",
    "pr_ht['body'] = pr_ht['body'].map(lambda x: BeautifulSoup(str(x),\"lxml\").text)\n",
    "# press releases can have the location of the press release at the beginning of the text\n",
    "pr_ht['body'] = pr_ht['body'].map(remove_location_at_beginning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_ht['body'].iloc[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical architecture for an information extraction system\n",
    "[Material is taken from here](http://www.nltk.org/book/ch07.html#ref-chunkex-cp)\n",
    "\n",
    "#### <center>Information Extraction Architecture</center>\n",
    "<img alt=\"Architecture\" src=\"resources/information_extraction_architecture.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation, Tokenization and Tagging Part of Speech\n",
    "* Let's perform the first three tasks\n",
    "* If you want, you can improve the standard models based on your corpora, especially the step # 3 POS tagging\n",
    "* If your corpora consists of 'new' words, the standard libraries could be inaccurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document) \n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "# create new columns\n",
    "pr_ht['pos body'] = pr_ht['body'].map(ie_preprocess)\n",
    "pr_ht['pos title'] = pr_ht['title'].map(ie_preprocess)\n",
    "pd.options.display.max_colwidth=100\n",
    "print(pr_ht[['pos body','pos title']].head())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of all possible Part of Speech tags in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-entity detection stage: Noun Phrase chunking or NP-chunking\n",
    "We will search for chunks corresponding to individual noun phrases\n",
    "<img alt=\"Chunk Segmentation\" src=\"resources/chunk-segmentation.png\" width=\"600px\" />\n",
    "<center>**Tree representation**</center>\n",
    "<img alt=\"Chunk Segmentation\" src=\"resources/chunk-tree.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based approach: NP-chunking with regular expressions\n",
    "* Let's define the rule (**chunk grammar**) that will divide the sentences into our NP-chunks based on POS tagging\n",
    "* Part-of-speech tags are delimited using angle brackets\n",
    "* This rule will chunk any sequence of tokens beginning with an optional determiner or possessive pronoun, followed by zero or more adjectives of any type (including relative adjectives like earlier/JJR), followed by one or more nouns of any type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the chunk grammar\n",
    "# in general case you want the chunk grammar as below\n",
    "#grammar = \"\"\"NP: {<DT|PRP\\$>?<JJ.*>*<NN.*>+}\"\"\"\n",
    "# for our case, let's have this one instead\n",
    "grammar = \"\"\"NP: {<JJ.*>*<NN.*>+}\"\"\"\n",
    "\n",
    "# Using the grammar, create a chunk parser\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "# Chunk the sentences based on your rule\n",
    "pr_ht['chunks'] = pr_ht['pos body'].map(lambda sentences: [cp.parse(sentence) for sentence in sentences])\n",
    "\n",
    "# Print one example\n",
    "pd.options.display.max_colwidth=1000\n",
    "print(pr_ht['body'].iloc[2])\n",
    "print(pr_ht['chunks'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here you can play around with chunk grammar to accurately determine NP-chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the index of the document you want to investigate\n",
    "i = 0\n",
    "\n",
    "# extract the document's raw text\n",
    "document = pr_ht['body'].iloc[i]\n",
    "print('Original document')\n",
    "print(document)\n",
    "print('------------------------------------------------')\n",
    "\n",
    "# process your document\n",
    "processed_document = nltk.sent_tokenize(document) \n",
    "processed_document = [nltk.word_tokenize(sent) for sent in processed_document]\n",
    "processed_document = [nltk.pos_tag(sent) for sent in processed_document]\n",
    "print('\\nProcessed document')\n",
    "print(processed_document)\n",
    "print('------------------------------------------------')\n",
    "\n",
    "# define the chunk rule, let's say everything will be NP if a noun preceded by ) or more adjectives  \n",
    "grammar = \"\"\"My Chunk. Yeah!: {<JJ*>*<NN*>+}\"\"\"\n",
    "\n",
    "# create a chunk parser\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "# chunk your document \n",
    "chunk_trees = [cp.parse(sent) for sent in processed_document]\n",
    "print('\\nYour Chunked Phrases')\n",
    "for tree in chunk_trees:\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'My Chunk. Yeah!': \n",
    "            print(subtree)\n",
    "print('------------------------------------------------')\n",
    "print('Visual representation (tree) of the document\\'s first sentence')\n",
    "#chunk_trees[0].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning approach: NP-chunking (I'll leave this as an exercise for you)\n",
    "* For ML approach, you need annotated data. Unfortunately DOJ isn't.\n",
    "* You can use existing annotated data like WSJ data in the NLTK package. Then you can assume that DOJ and this annotated data are from the same distribution and the models you built on the annotated data will work fine for DOJ data\n",
    "* Simple ML model can be built by using n-gram tagger to label sentences with chunk tags [part 3.2](http://www.nltk.org/book/ch07.html#ref-chunkex-cp)\n",
    "* More comprehensive ML model would be a classifier-based chunkers such as the ConsecutiveNPChunker [part 3.3](http://www.nltk.org/book/ch07.html#ref-chunkex-cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the category of Human Trafficking\n",
    "<img alt=\"Example HT\" src=\"resources/HT_example1.png\" width=\"600px\" />\n",
    "\n",
    "* Usually titles are very informative stating the gender, location of a trafficker and the category of Human Trafficking\n",
    "* The first paragraph is usually more relevant to an incident than the next ones. The last paragraph usually has general information on Human Trafficking rather than information related to the incident (**task: divide a document onto paragraphs**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Detection - Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_trafficking_category(chunk_trees):\n",
    "    categories = []\n",
    "    for tree in chunk_trees:\n",
    "        for subtree in tree.subtrees():\n",
    "            # find all NP-chunk\n",
    "            if subtree.label() == 'NP':\n",
    "                leaves = [w[0] for w in subtree.leaves()]\n",
    "                # if an NP-chunk has trafficking in it, then we start extracting category\n",
    "                if 'trafficking' in leaves:\n",
    "                    # clean up and creating a string out of a list\n",
    "                    c = ' '.join([re.sub('[^a-z]','',x.lower()) for x in leaves if not re.search('human',x,re.I)])\n",
    "                    if re.sub(r'\\s','',c) != '':\n",
    "                        categories.append(re.sub('(^\\s+|\\s+$)','',c))\n",
    "                        \n",
    "    categories2 = Counter(categories)\n",
    "    keys = []\n",
    "    for key in categories2.keys():\n",
    "        if len(keys) == 0:\n",
    "            keys.append(key)\n",
    "        else:\n",
    "            for i,k in enumerate(keys):\n",
    "                if key in k:\n",
    "                    break\n",
    "                elif k in key:\n",
    "                    keys[i] = key\n",
    "                    break\n",
    "            if len(keys) == i+1:\n",
    "                keys.append(key)\n",
    "                \n",
    "    new_categories = dict((k, 0) for k in keys)  \n",
    "    for key in categories2:\n",
    "        for key1 in new_categories:\n",
    "            if key in key1:\n",
    "                new_categories[key1] += categories2[key]\n",
    "                break\n",
    "    \n",
    "    new_categories = sorted(new_categories, key=new_categories.get) \n",
    "    if len(new_categories):\n",
    "        category = new_categories[-1]\n",
    "        if category == 'trafficking':\n",
    "            category = ''\n",
    "    else:\n",
    "        category = ''\n",
    "    return(category)            \n",
    " \n",
    "#print(human_trafficking_category(pr_ht['chunks'].iloc[10]))    \n",
    "pr_ht['category'] = pr_ht['chunks'].map(human_trafficking_category)  \n",
    "print(pr_ht['category'].head(50))\n",
    "print('# of articles that have a category:',len(pr_ht[pr_ht['category']!='']),\\\n",
    "      round(100*len(pr_ht[pr_ht['category']!=''])/len(pr_ht),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Human Trafficking Category Through Spacy\n",
    "Let's use dependency parsing method in Spacy to detect categories of Human Trafficking\n",
    "This is a big model and very resource consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "environment_level = int(environ.get(\n",
    "    'CROSSCOMPUTE_ENVIRONMENT_LEVEL', 0))\n",
    "memory_level = int(environ.get(\n",
    "    'CROSSCOMPUTE_MEMORY_LEVEL'))\n",
    "\n",
    "if environment_level < 1:\n",
    "    print(\n",
    "        'environment_level.error = environment level must be set to '\n",
    "        'computational in order to use the spacy package because it '\n",
    "        'takes too long to install')\n",
    "\n",
    "if memory_level < 3:\n",
    "    print(\n",
    "        'memory_level.error = memory level should be set to large '\n",
    "        'or higher in order to use the en_core_web_lg spacy model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load spacy's large model (be careful it has size of 812 MB!)\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def type_ht_spacy (document, token, ptag):    \n",
    "    types = []\n",
    "    for chunk in document.noun_chunks:\n",
    "        if re.search(token,chunk.root.text, re.I):\n",
    "            doc = nlp(chunk.text)\n",
    "            for word in doc:\n",
    "                if word.pos_ == ptag and not re.search(\n",
    "                    token,word.text,re.I\n",
    "                ) and not re.search('human',word.lemma_):\n",
    "                    types.append(word.lemma_)               \n",
    "    return Counter(types)\n",
    "\n",
    "pr_ht['type ht spacy'] = pr_ht['body'].map(\n",
    "    lambda x: type_ht_spacy(nlp(x),'traffic','NOUN'))   \n",
    "print(pr_ht['type ht spacy'])\n",
    "print('# of articles with a category:',len(pr_ht[pr_ht[\n",
    "    'type ht spacy'] != {}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# of articles with a category:',len(pr_ht[pr_ht['type ht spacy'] != {}]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
