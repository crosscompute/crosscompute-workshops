{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract a Table of Human Trafficking Incidents from a Table of USA DOJ Court Case Press Releases using NLTK\n",
    "\n",
    "We presented this [tool](https://crosscompute.com/t/XlntAt807TmTj6YWfrTW2Gis3FvC2nPJ) and [notebook](https://crosscompute.com/n/BhkK4AlpPD4Hmn8O0mbDtrH0q4HzphfN/-/extract-human-trafficking-incidents-from-court-cases-using-nltk) as part of our workshop on [Computational Approaches to Fight Human Trafficking](https://www.meetup.com/spatiotemporal-analysis-for-community-health-and-safety/events/244179401). Thanks to Aida Shoydokova for writing much of the original code.\n",
    "\n",
    "- This code may take a long time to run. We recommend reserving an execution time of at least **10 minutes**, depending on the size of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossCompute\n",
    "usa_doj_press_release_table_path = (\n",
    "    'human-trafficking-usa-doj-20171111-1730-sample-30.csv')\n",
    "keyword_prefix = 'traffick'\n",
    "category_text_path = 'human-trafficking-categories.txt'\n",
    "target_folder = '/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    import pip\n",
    "    pip.main(['install', 'nltk'])\n",
    "    import nltk\n",
    "    \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t = pd.read_csv(usa_doj_press_release_table_path)\n",
    "print('document_count = %s' % len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [x.lower() for x in open(\n",
    "    category_text_path).read().splitlines()]\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Documents Related to Human Trafficking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PATTERN_KEYWORD = re.compile(r'human\\s+traffick', re.IGNORECASE)\n",
    "\n",
    "def has_right_topic(topic_names):\n",
    "    if pd.isnull(topic_names):\n",
    "        return False\n",
    "    for topic_name in topic_names.split(';'):\n",
    "        if PATTERN_KEYWORD.search(topic_name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "t['has_right_topic'] = t['topic_names'].map(has_right_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of documents with the right topic\n",
    "print('matching_topic_document_count = %s' % t[\n",
    "    'has_right_topic'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_empty_topic_but_right_text(row):\n",
    "    topic_names = row['topic_names']\n",
    "    if not pd.isnull(topic_names):\n",
    "        return False\n",
    "    title = row['title']\n",
    "    if not pd.isnull(title) and PATTERN_KEYWORD.search(title):\n",
    "        return True\n",
    "    body = row['body']\n",
    "    if not pd.isnull(body) and PATTERN_KEYWORD.search(body):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "t['has_empty_topic_but_right_text'] = t.apply(\n",
    "    has_empty_topic_but_right_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of documents with empty topic but right text\n",
    "print('empty_topic_matching_text_document_count = %s' % t[\n",
    "    'has_empty_topic_but_right_text'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of human trafficking court cases\n",
    "selected_t = t[\n",
    "    t.has_right_topic | t.has_empty_topic_but_right_text].copy()\n",
    "print('human_trafficking_document_count = %s' % len(selected_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "NEWLINE_PATTERN = re.compile(r'\\n+', re.MULTILINE)\n",
    "\n",
    "def get_body_text(x):\n",
    "    if not hasattr(x, 'getText'):\n",
    "        x = BeautifulSoup(x, 'lxml')\n",
    "    # Extract text without tags\n",
    "    text = x.getText(separator=u'\\n').strip()\n",
    "    # Replace multiple newlines with a single newline\n",
    "    text = NEWLINE_PATTERN.sub('\\n', text)\n",
    "    return text\n",
    "\n",
    "selected_t['body_text'] = selected_t['body'].map(get_body_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_part_of_speech(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [nltk.word_tokenize(x) for x in sentences]\n",
    "    sentences = [nltk.pos_tag(x) for x in sentences]\n",
    "    return sentences\n",
    "\n",
    "tag_part_of_speech('Here I am.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def combine_matches(count_by_x):\n",
    "    'Combine first or last name counts with full name counts'\n",
    "    d = dict(count_by_x)\n",
    "    for x1, x2 in combinations(d.keys(), 2):\n",
    "        if x1 in x2:\n",
    "            abridged_x = x1\n",
    "            full_x = x2\n",
    "        elif x2 in x1:\n",
    "            abridged_x = x2\n",
    "            full_x = x1    \n",
    "        else:\n",
    "            continue\n",
    "        d[full_x] += d[abridged_x]\n",
    "        d[abridged_x] = 0\n",
    "    return d\n",
    "\n",
    "combine_matches({\n",
    "    'benjamin b wagner': 1,\n",
    "    'benjamin': 3,\n",
    "    'anthony w ishii': 2,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_count_table = pd.read_csv(\n",
    "    'names-usa.csv.xz', compression='xz', index_col=0)\n",
    "\n",
    "def get_gender(name):\n",
    "    if not name:\n",
    "        return\n",
    "    given_name = name.split()[0].lower()\n",
    "    try:\n",
    "        selected_table = name_count_table.loc[given_name]\n",
    "    except KeyError:\n",
    "        return\n",
    "    return selected_table.idxmax()\n",
    "\n",
    "get_gender('jay leno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_category_with_nltk(tagged_sentences):\n",
    "    parse = nltk.RegexpParser('WHEE: {<JJ.*>*<NN.*>+}').parse\n",
    "    vote_by_category = defaultdict(int)\n",
    "    vote_weight = 1  # Count votes less if the term appears later\n",
    "    for sentence in tagged_sentences:\n",
    "        chunk_tree = parse(sentence)\n",
    "        for tree in chunk_tree.subtrees():\n",
    "            if tree.label() != 'WHEE':\n",
    "                continue\n",
    "            terms = [x[0].lower() for x in tree.leaves()]\n",
    "            if not has_keyword(terms):\n",
    "                continue\n",
    "            for x in categories:\n",
    "                if x in terms:\n",
    "                    vote_by_category[x] += vote_weight\n",
    "                    vote_weight *= 0.9\n",
    "    if not vote_by_category:\n",
    "        return\n",
    "    return pd.Series(vote_by_category).idxmax()\n",
    "\n",
    "def has_keyword(terms):\n",
    "    for x in terms:\n",
    "        if x.startswith(keyword_prefix):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "get_category_with_nltk(tag_part_of_speech(\n",
    "    'labor trafficker'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def extract_information_with_nltk(row):\n",
    "    tagged_sentences = tag_part_of_speech(row.body_text)\n",
    "    places, names = [], []\n",
    "    for sentence in tagged_sentences:\n",
    "        chunk_tree = nltk.ne_chunk(sentence)\n",
    "        for tree in chunk_tree.subtrees():\n",
    "            label = tree.label()\n",
    "            x = ' '.join(x[0] for x in tree.leaves())\n",
    "            x = x.lower().replace('.', '').strip()\n",
    "            if len(x) < 2:\n",
    "                continue  # Skip single characters\n",
    "            if label == 'GPE':\n",
    "                places.append(x)    \n",
    "            elif label == 'PERSON':\n",
    "                names.append(x)\n",
    "                \n",
    "    if places:\n",
    "        count_by_place = combine_matches(Counter(places))\n",
    "        place = pd.Series(count_by_place).idxmax()\n",
    "    else:\n",
    "        place = None\n",
    "        \n",
    "    if names:\n",
    "        count_by_name = combine_matches(Counter(names))\n",
    "        name = pd.Series(count_by_name).idxmax()\n",
    "    else:\n",
    "        name = None\n",
    "        \n",
    "    return pd.Series({\n",
    "        'place': place,\n",
    "        'name': name,\n",
    "        'gender': get_gender(name),\n",
    "        'category': get_category_with_nltk(tagged_sentences),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = target_folder + '/incidents.csv'\n",
    "extracted_t = selected_t.apply(\n",
    "    extract_information_with_nltk, axis=1)\n",
    "extracted_t.to_csv(target_path, index = False)\n",
    "print('incident_table_path = %s' % target_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
